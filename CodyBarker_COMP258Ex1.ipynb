{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766f0a7f",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "Write a Python program to classify letters from different fonts using delta rule learning. The\n",
    "training set contains 21 input vectors (patterns)....In other words, we train the perceptron to classify each of these vectors as belonging, or not\n",
    "belonging, to the class A (or B, C, D as stated above). In that case, the target value for each\n",
    "pattern is either 1 or - 1; only the first component of the target vector shown is applicable. The\n",
    "net is as shown below, and n = 63\n",
    "There are three examples of A and 18 examples of not-A in the training set.\n",
    "a) First, classify the input using perceptron.\n",
    "b) Then, classify the input using ADALINE and compare the ability of the trained\n",
    "ADALINE to classify noisy input to the results for the perceptron. Try 5, 10, 15, 20\n",
    "pixels wrong and the same levels of missing data\n",
    "Your output should display the results of classification in a friendly format and state the\n",
    "differences between two algorithms regarding handling noise and missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f49cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Results\n",
      "Letter K: Perceptron → Not A, ADALINE → Not A, True → Not A\n",
      "Letter B: Perceptron → Not A, ADALINE → Not A, True → Not A\n",
      "Letter D: Perceptron → Not A, ADALINE → Not A, True → Not A\n",
      "Letter E: Perceptron → Not A, ADALINE → Not A, True → Not A\n",
      "Letter B: Perceptron → Not A, ADALINE → Not A, True → Not A\n",
      "Letter C: Perceptron → Not A, ADALINE → A, True → Not A\n",
      "Letter D: Perceptron → Not A, ADALINE → Not A, True → Not A\n",
      "\n",
      "Summary:\n",
      "Perceptron Accuracy: 100.00%\n",
      "ADALINE Accuracy:    85.71%\n",
      "\n",
      "Noise level 5 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'9992, 'Not A:'8\n",
      "  ADALINE    → 'A:'8082, 'Not A:'1918\n",
      "Missing 5 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'10000, 'Not A:'0\n",
      "  ADALINE    → 'A:'9648, 'Not A:'352\n",
      "\n",
      "Noise level 10 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'9658, 'Not A:'342\n",
      "  ADALINE    → 'A:'7062, 'Not A:'2938\n",
      "Missing 10 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'10000, 'Not A:'0\n",
      "  ADALINE    → 'A:'8949, 'Not A:'1051\n",
      "\n",
      "Noise level 15 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'8743, 'Not A:'1257\n",
      "  ADALINE    → 'A:'6500, 'Not A:'3500\n",
      "Missing 15 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'9997, 'Not A:'3\n",
      "  ADALINE    → 'A:'8375, 'Not A:'1625\n",
      "\n",
      "Noise level 20 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'7659, 'Not A:'2341\n",
      "  ADALINE    → 'A:'6083, 'Not A:'3917\n",
      "Missing 20 pixels (over 10000 trials):\n",
      "  Perceptron → 'A:'9968, 'Not A:'32\n",
      "  ADALINE    → 'A:'7987, 'Not A:'2013\n"
     ]
    }
   ],
   "source": [
    "#Cody Barker 301441462\n",
    "#Imports!\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "    \n",
    "# converts a list of 0s and 2s to a list of -1s and 1s, do not use as copied for doc we were given but this is the fucntion that would convert   \n",
    "def bipolar(vec) -> List[int]:\n",
    "    return [p - 1 for p in vec]\n",
    "\n",
    "# converts a list of vectors to a row of integers. Flattens matrix into vector\n",
    "def flatten_grid(pattern) -> List[int]:\n",
    "    flat = []\n",
    "    for row in pattern:\n",
    "        for pix in row:\n",
    "            flat.append(pix)\n",
    "    return flat\n",
    "\n",
    "# flip bits to create noise\n",
    "def add_noise(vector, num_flips) -> List[int]:\n",
    "    noisy = vector.copy()\n",
    "    flip_indices = random.sample(range(len(noisy)), num_flips)\n",
    "    for i in flip_indices:\n",
    "        noisy[i] *= -1\n",
    "    return noisy\n",
    "\n",
    "# adds missing value to data\n",
    "def add_missing_data(vector, num_missing) -> List[int]:\n",
    "    missing = vector.copy()\n",
    "    missing_indices = random.sample(range(len(missing)), num_missing)\n",
    "    for i in missing_indices:\n",
    "        missing[i] = 0 \n",
    "    return missing\n",
    "    \n",
    "# make patterns for letters A, B, C, D, E, K, J, Copied from doc we were given\n",
    "def make_letter_patterns():\n",
    "    \n",
    "    A1 = [\n",
    "        [-1, -1,  1,  1, -1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1,  1, -1, -1],\n",
    "        [-1, -1,  1, -1,  1, -1, -1],\n",
    "        [-1,  1,  1,  1,  1,  1, -1],\n",
    "        [-1,  1, -1, -1, -1,  1, -1],\n",
    "        [-1,  1, -1, -1, -1,  1, -1],\n",
    "        [ 1,  1,  1, -1,  1,  1,  1],\n",
    "    ]\n",
    "    \n",
    "    A2 = [\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1,  1, -1, -1],\n",
    "        [-1, -1,  1, -1,  1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1,  1, -1],\n",
    "        [-1,  1,  1,  1,  1,  1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    ]\n",
    "    A3 = [\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1,  1, -1, -1],\n",
    "        [-1, -1,  1, -1,  1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1,  1, -1],\n",
    "        [-1,  1,  1,  1,  1,  1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1,  1, -1, -1, -1,  1,  1],\n",
    "    ]\n",
    "    \n",
    "    B1 = [\n",
    "        [ 1,  1,  1,  1,  1,  1, -1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1,  1,  1,  1,  1, -1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],  \n",
    "        [ 1,  1,  1,  1,  1,  1, -1],  \n",
    "    ]\n",
    "    \n",
    "    B2 = [\n",
    "        [ 1,  1,  1,  1,  1,  1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1,  1,  1,  1,  1,  1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1,  1,  1,  1,  1,  1, -1],\n",
    "    ]\n",
    "    \n",
    "    B3 = [\n",
    "        [ 1,  1,  1,  1,  1,  1, -1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1,  1,  1,  1,  1, -1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [ 1,  1,  1,  1,  1,  1, -1],\n",
    "    ]\n",
    "\n",
    "    C1 = [\n",
    "        [-1, -1,  1,  1,  1,  1,  1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1, -1,  1],\n",
    "        [-1, -1,  1,  1,  1,  1, -1],\n",
    "    ]\n",
    "    \n",
    "    C2 = [\n",
    "        [-1, -1,  1,  1,  1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1,  1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1, -1,  1, -1],\n",
    "        [-1, -1,  1,  1,  1, -1, -1],\n",
    "    ]\n",
    "\n",
    "    C3 = [\n",
    "    [-1, -1,  1,  1,  1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1,  1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1,  1,  1],\n",
    "    [-1, -1,  1,  1,  1, -1, -1],\n",
    "]\n",
    "    \n",
    "    D1 = [\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1,  1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1,  1, -1],\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "]\n",
    "\n",
    "    D2 = [\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1,  1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1,  1],\n",
    "    [ 1, -1, -1, -1, -1,  1, -1],\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "]\n",
    "\n",
    "    D3 = [\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "]\n",
    "\n",
    "    E1 = [\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1,  1,  1,  1,  1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1,  1,  1,  1,  1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1,  1],\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "]\n",
    "\n",
    "    E2 = [\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1,  1,  1,  1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1, -1, -1],\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "]\n",
    "    \n",
    "    E3 = [\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "    [-1,  1, -1, -1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1, -1, -1],\n",
    "    [-1,  1,  1,  1,  1,  1, -1],\n",
    "    [-1,  1, -1, -1, -1, -1, -1],\n",
    "    [-1,  1,  1,  1,  1,  1, -1],\n",
    "    [-1,  1, -1, -1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1, -1, -1],\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "]\n",
    "    \n",
    "    K1 = [\n",
    "    [ 1,  1,  1, -1, -1,  1,  1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1,  1, -1, -1, -1, -1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1,  1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [ 1,  1, -1, -1, -1, -1,  1],\n",
    "]\n",
    "\n",
    "    K2 = [\n",
    "    [ 1, -1, -1, -1, -1,  1, -1],\n",
    "    [ 1, -1, -1,  1, -1, -1, -1],\n",
    "    [ 1, -1,  1, -1,  1, -1, -1],\n",
    "    [ 1,  1, -1, -1,  1, -1, -1],\n",
    "    [ 1,  1, -1,  1, -1, -1, -1],\n",
    "    [ 1, -1,  1, -1,  1, -1, -1],\n",
    "    [ 1, -1, -1,  1, -1,  1, -1],\n",
    "    [ 1, -1, -1, -1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1, -1,  1, -1],\n",
    "]\n",
    "\n",
    "    K3 = [\n",
    "    [ 1,  1,  1, -1, -1,  1,  1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1,  1, -1, -1, -1, -1],\n",
    "    [-1,  1, -1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1,  1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [ 1,  1, -1, -1, -1, -1,  1],\n",
    "]\n",
    "\n",
    "    J1 = [\n",
    "    [ 1,  1,  1,  1,  1,  1,  1],\n",
    "    [-1, -1, -1, -1,  1, -1, -1],\n",
    "    [-1, -1, -1, -1,  1, -1, -1],\n",
    "    [-1, -1, -1, -1,  1, -1, -1],\n",
    "    [-1, -1, -1, -1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1,  1, -1, -1],\n",
    "    [ 1, -1, -1, -1,  1, -1, -1],\n",
    "    [-1,  1,  1,  1, -1, -1, -1],\n",
    "]\n",
    "    \n",
    "    J2 = [\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1,  1,  1,  1, -1, -1],\n",
    "]\n",
    "    \n",
    "    J3 = [\n",
    "    [-1, -1, -1, -1, -1,  1,  1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1,  1, -1],\n",
    "    [-1,  1, -1, -1, -1,  1, -1],\n",
    "    [-1, -1,  1,  1,  1, -1, -1],\n",
    "]\n",
    "\n",
    "    return {\n",
    "    'A': [A1, A2, A3],\n",
    "    'B': [B1, B2, B3],\n",
    "    'C': [C1, C2, C3],\n",
    "    'D': [D1, D2, D3],\n",
    "    'E': [E1, E2, E3],\n",
    "    'K': [K1, K2, K3],\n",
    "    'J': [J1, J2, J3],\n",
    "}\n",
    "\n",
    "class Perceptron:\n",
    "    #Perceptron class with methods for training and prediction\n",
    "    def __init__(self, input_size: int, learning_rate: float = 0.01, epochs: int = 20):\n",
    "        # initialize weights and bias\n",
    "        self.weights = [random.uniform(-0.5, 0.5) for _ in range(input_size)]\n",
    "        self.bias = random.uniform(-0.5, 0.5)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def activation(self, x: float) -> int:\n",
    "        return 1 if x >= 0 else -1\n",
    "    \n",
    "    def predict(self, x: List[int]) -> int:\n",
    "        total = sum(w * xi for w, xi in zip(self.weights, x)) + self.bias\n",
    "        return self.activation(total)\n",
    "    \n",
    "    def train(self, X: List[List[int]], y: List[int]) -> None:\n",
    "        for epoch in range(self.epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                prediction = self.predict(xi)\n",
    "                error = target - prediction\n",
    "                self.weights = [w + self.learning_rate * error * xi for w, xi in zip(self.weights, xi)]\n",
    "                self.bias += self.learning_rate * error\n",
    "  \n",
    "class Adaline:\n",
    "    def __init__(self, input_size: int, learning_rate: float = 0.01, epochs: int = 20):\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, input_size)\n",
    "        self.bias = random.uniform(-0.5, 0.5)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def activation(self, x: float) -> int:\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    \n",
    "    def train(self, X: List[List[int]], y: List[int]) -> None:\n",
    "        for epoch in range(self.epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                input = np.dot(xi, self.weights) + self.bias\n",
    "                error = target - input\n",
    "                # weight update\n",
    "                self.weights += self.learning_rate * error * np.array(xi) \n",
    "                self.bias += self.learning_rate * error\n",
    "                \n",
    "    def predict(self, x: List[int]) -> int:\n",
    "        total = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation(total)\n",
    "    \n",
    "def part_a(patterns, train_ratio = 0.7):\n",
    "    X, y, labels = [], [], []\n",
    "\n",
    "    # flatten all patterns and assign labels\n",
    "    for letter, variations in patterns.items():\n",
    "        for pattern in variations:\n",
    "            X.append(flatten_grid(pattern))\n",
    "            y.append(1 if letter == 'A' else -1)\n",
    "            labels.append(letter)\n",
    "\n",
    "    X, y, labels = np.array(X), np.array(y), np.array(labels)\n",
    "\n",
    "    # ensure at least 2 A's in training\n",
    "    indices = list(range(len(X)))\n",
    "    A_indices = [i for i, lbl in enumerate(labels) if lbl == 'A']\n",
    "    notA_indices = [i for i, lbl in enumerate(labels) if lbl != 'A']\n",
    "\n",
    "    # pick at least 2 A's for training\n",
    "    chosen_A_train = random.sample(A_indices, 2)\n",
    "    remaining_indices = [i for i in indices if i not in chosen_A_train]\n",
    "\n",
    "    # shuffle the remaining indices\n",
    "    random.shuffle(remaining_indices)\n",
    "\n",
    "    # calculate how many total go into training\n",
    "    split_idx = int(len(X) * train_ratio)\n",
    "\n",
    "    # fill training set with chosen A's + rest until split\n",
    "    final_train_idx = chosen_A_train + remaining_indices[:split_idx - len(chosen_A_train)]\n",
    "    final_test_idx = remaining_indices[split_idx - len(chosen_A_train):]\n",
    "\n",
    "    # build train/test sets\n",
    "    train_X, test_X = X[final_train_idx], X[final_test_idx]\n",
    "    train_y, test_y = y[final_train_idx], y[final_test_idx]\n",
    "    train_labels, test_labels = labels[final_train_idx], labels[final_test_idx]\n",
    "\n",
    "    # train Perceptron\n",
    "    perceptron = Perceptron(input_size=train_X.shape[1], learning_rate=0.5, epochs=20)\n",
    "    perceptron.train(train_X, train_y)\n",
    "\n",
    "    # train Adaline\n",
    "    adaline = Adaline(input_size=train_X.shape[1], learning_rate=0.01, epochs=150)\n",
    "    adaline.train(train_X, train_y)\n",
    "\n",
    "    # test both models\n",
    "    print(\"Classification Results\")\n",
    "    correct_p, correct_a = 0, 0\n",
    "    for x, true_label, letter in zip(test_X, test_y, test_labels):\n",
    "        p_pred = perceptron.predict(x)\n",
    "        a_pred = adaline.predict(x)\n",
    "        print(f\"Letter {letter}: Perceptron → {'A' if p_pred == 1 else 'Not A'}, \"\n",
    "              f\"ADALINE → {'A' if a_pred == 1 else 'Not A'}, \"\n",
    "              f\"True → {'A' if true_label == 1 else 'Not A'}\")\n",
    "\n",
    "        if p_pred == true_label:\n",
    "            correct_p += 1\n",
    "        if a_pred == true_label:\n",
    "            correct_a += 1\n",
    "\n",
    "    # print accuracy\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Perceptron Accuracy: {correct_p / len(test_y) * 100:.2f}%\")\n",
    "    print(f\"ADALINE Accuracy:    {correct_a / len(test_y) * 100:.2f}%\")\n",
    "    \n",
    "    \n",
    "def part_b(patterns):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    \n",
    "    for letter, variation in patterns.items():\n",
    "        for pattern in variation:\n",
    "            X.append(flatten_grid(pattern))\n",
    "            \n",
    "            if letter == 'A':\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(-1)\n",
    "                \n",
    "    X, y = np.array(X), np.array(y)     \n",
    "    # perceptron train and test\n",
    "    perceptron = Perceptron(input_size=X.shape[1], learning_rate=0.5, epochs=20)\n",
    "    perceptron.train(X, y)\n",
    "    \n",
    "    # TRain Adaline\n",
    "    adaline = Adaline(input_size=X.shape[1], learning_rate=0.01, epochs=150)\n",
    "    adaline.train(X, y)\n",
    "    \n",
    "    clean_A = flatten_grid(patterns['A'][0])\n",
    "    \n",
    "    for noise_level in [5, 10, 15, 20]:\n",
    "        perceptron_noise_results = []\n",
    "        adaline_noise_results = []\n",
    "        perceptron_missing_results = []\n",
    "        adaline_missing_results = []\n",
    "        trials = 10000\n",
    "        \n",
    "        for _ in range(trials):\n",
    "            noisy = add_noise(clean_A, noise_level)\n",
    "            missing = add_missing_data(clean_A, noise_level)\n",
    "            perceptron_noise_results.append(perceptron.predict(noisy))\n",
    "            adaline_noise_results.append(adaline.predict(noisy))\n",
    "            perceptron_missing_results.append(perceptron.predict(missing))\n",
    "            adaline_missing_results.append(adaline.predict(missing)) \n",
    "        \n",
    "        print(f\"\\nNoise level {noise_level} pixels (over {trials} trials):\")\n",
    "        print(f\"  Perceptron → 'A:'{perceptron_noise_results.count(1)}, 'Not A:'{perceptron_noise_results.count(-1)}\")\n",
    "        print(f\"  ADALINE    → 'A:'{adaline_noise_results.count(1)}, 'Not A:'{adaline_noise_results.count(-1)}\")\n",
    "        print(f\"Missing {noise_level} pixels (over {trials} trials):\")\n",
    "        print(f\"  Perceptron → 'A:'{perceptron_missing_results.count(1)}, 'Not A:'{perceptron_missing_results.count(-1)}\")\n",
    "        print(f\"  ADALINE    → 'A:'{adaline_missing_results.count(1)}, 'Not A:'{adaline_missing_results.count(-1)}\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    patterns = make_letter_patterns()\n",
    "    part_a(patterns)\n",
    "    part_b(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029644d",
   "metadata": {},
   "source": [
    "We can see from the results and run again multiple times to confirm. Perceptron uses a hard threshold acitvation, as long as ther weighted sum crossse the threshold the exact magnitude does not matter. Therefore it can handle losing a few pixels, and can tolerate a bit of noise, but fails when the noise is strong enough to flip it. Adaline uses delta rule and considers the magnitude of the acitvation before applying the step function. This allows for more precise weights during training, but even small noise or missing pixels shift the weighted sum, as the model is mor e senstive. I a test I did, for 5 pixels missing or noise perceptron as still able to identify most correct values, however as more and more nosie or missin pixels were added the percentage of correct values was lowered. For Adaline even for low amounts of noise or missing values it did not have as good of a correct percentage as Perceptron. But Adaline handled noise and missing values similar with similar correct percentages, while perceptron handled noise better tha it is able to handle missing vlaues. Perceptron is better at handling disortions, while adaline is more sensitive, however it provides a more prinicpled learnign rule that works better when the data is consistent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
